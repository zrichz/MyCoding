{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A matching Triton is not available, some optimizations will not be enabled\n",
            "Traceback (most recent call last):\n",
            "    from xformers.triton.softmax import softmax as triton_softmax  # noqa\n",
            "    import triton\n",
            "ModuleNotFoundError: No module named 'triton'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "#import libraries\n",
        "import os\n",
        "import time \n",
        "import argparse\n",
        "from PIL import Image, ImageChops\n",
        "\n",
        "from diffusers import AutoPipelineForText2Image\n",
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "    \n",
        "import torch\n",
        "import imageio\n",
        "import sys      # provides access to some vars used or maintained by Python interpreter, and to functions that interact with it.\n",
        "\n",
        "from xformers.ops import MemoryEfficientAttentionFlashAttentionOp # Enable memory efficient attention from xFormers.(not working atm)\n",
        "\n",
        "import numpy as np   # for 3D rotate\n",
        "import scipy.ndimage # for 3D rotate\n",
        "\n",
        "#check cuda availability\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print('Using device:', device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#define model and pipeline\n",
        "\n",
        "#optimisation trick #1 - Use TensorFloat-32\n",
        "'''On Ampere and later CUDA devices, matrix multiplications\n",
        " and convolutions can use the TensorFloat-32 (TF32) mode for\n",
        " faster, but slightly less accurate computations. \n",
        " By default, PyTorch enables TF32 mode for convolutions,\n",
        " but NOT for matmul. enabling it can significantly speed up\n",
        " computations with minimal loss in numerical accuracy.\n",
        "'''\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "# note just this change improved inference speed by 24% on RTX 3070\n",
        "\n",
        "args = None\n",
        "\n",
        "# path to SD1.5 model\n",
        "SD15_MODEL = \"D:\\_DUPLICATED\\_duplicatedOnSSDgate_1TB\\SD_CKPTS_used_by_Auto1111\\realisticVisionV51_v51VAE.safetensors\"\n",
        "LOOP_COUNT = 4 # Number of times to loop the final animation\n",
        "\n",
        "# Define pipeline (half-precision weights for faster execution)\n",
        "# def get_pipeline():\n",
        "#     return (\n",
        "#         StableDiffusionImg2ImgPipeline.from_single_file(\n",
        "#             SD15_MODEL, torch_dtype=torch.float16\n",
        "#         ).to(\"cuda\"),\n",
        "        \n",
        "#         AutoPipelineForText2Image.from_pretrained(\"runwayml/stable-diffusion-v1-5\",\n",
        "#             torch_dtype=torch.float16,\n",
        "#             variant=\"fp16\", use_safetensors=True, safety_checker=None,\n",
        "#         ).to(\"cuda\")\n",
        "#     )\n",
        "\n",
        "def get_pipeline():\n",
        "    return (\n",
        "        StableDiffusionImg2ImgPipeline.from_single_file( SD15_MODEL, torch_dtype=torch.float16 ).to(\"cuda\"),\n",
        "        \n",
        "        AutoPipelineForText2Image.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16,\n",
        "            variant=\"fp16\", use_safetensors=True, safety_checker=None,\n",
        "        ).to(\"cuda\")\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to Generate the image\n",
        "\n",
        "def fn_generate(input_path, prompt, pipeline):\n",
        "    init_image = None\n",
        "    if input_path:\n",
        "        # Load the input image\n",
        "        init_image = Image.open(input_path).convert(\"RGB\")\n",
        "        \n",
        "    # Generate the output image\n",
        "    return pipeline( prompt=prompt, image=init_image, strength=args.strength,\n",
        "        num_inference_steps=args.num_inference_steps, guidance_scale=args.guidance_scale,\n",
        "        generator=( torch.Generator(device=\"cuda\").manual_seed(args.seed) if args.seed else None ) ).images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like (None,) is not the path to a directory containing a file named None or \nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/diffusers/installation#offline-mode'.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: '(None,)'.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipeline for img2img has issues\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mfn_check_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m     \u001b[38;5;66;03m#test the pipeline\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[4], line 4\u001b[0m, in \u001b[0;36mfn_check_pipeline\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn_check_pipeline\u001b[39m():\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Check if the pipeline is working\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     img2img_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mget_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m#print(f\"img2img_pipeline: {img2img_pipeline}\")\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg2img_pipeline: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(img2img_pipeline)[:\u001b[38;5;241m100\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# print the first 100 characters of the pipeline\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[2], line 35\u001b[0m, in \u001b[0;36mget_pipeline\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_pipeline\u001b[39m():\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m---> 35\u001b[0m         \u001b[43mStableDiffusionImg2ImgPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_single_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mSD15_MODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     36\u001b[0m         \n\u001b[0;32m     37\u001b[0m         AutoPipelineForText2Image\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunwayml/stable-diffusion-v1-5\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[0;32m     38\u001b[0m             variant\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_safetensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, safety_checker\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     39\u001b[0m         )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m     )\n",
            "\u001b[1;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like (None,) is not the path to a directory containing a file named None or \nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/diffusers/installation#offline-mode'."
          ]
        }
      ],
      "source": [
        "def fn_check_pipeline():\n",
        "    # Check if the pipeline is working\n",
        "    \n",
        "    img2img_pipeline = get_pipeline()\n",
        "    #print(f\"img2img_pipeline: {img2img_pipeline}\")\n",
        "    print(f\"img2img_pipeline: {str(img2img_pipeline)[:100]}\") # print the first 100 characters of the pipeline\n",
        "    \n",
        "    if img2img_pipeline is None:\n",
        "        raise ValueError(\"pipeline for img2img has issues\")\n",
        "\n",
        "    return\n",
        "\n",
        "fn_check_pipeline()     #test the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#create argument parser\n",
        "class Args:\n",
        "    init_image = None\n",
        "    output_path = \"output-frames\"\n",
        "    count = 50  # number of frames to generate\n",
        "    prompt = \"a strange being, solo, male, wearing light brown sackcloth, 21st century, style of Hieronymus Bosch, spurious, rotors, floral background, flowers, flowery background\"  # replace with your value\n",
        "    strength = 0.65\n",
        "    seed = None\n",
        "    num_inference_steps = 10\n",
        "    guidance_scale = 7\n",
        "    skip_frame_generation = False\n",
        "    gif = True\n",
        "    no_interpolate = True # disable motion interpolation for final ffmpeg post-processing\n",
        "    model = \"SD15\"\n",
        "\n",
        "args = Args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# main function to generate the images\n",
        "\n",
        "if not args.skip_frame_generation:\n",
        "    img2img_pipeline, text_pipeline = get_pipeline()\n",
        "\n",
        "    if not os.path.exists(args.output_path):\n",
        "        os.makedirs(args.output_path)\n",
        "\n",
        "    if args.init_image:\n",
        "        # get initial image\n",
        "        src_image = args.init_image\n",
        "\n",
        "        # copy the initial image to the output path as frame 0\n",
        "        init_image = Image.open(src_image).convert(\"RGB\")\n",
        "        #init_image = init_image.resize((1024, 1024))\n",
        "        init_image.save(os.path.join(args.output_path, f'frame_{\"0\".zfill(4)}.png'))\n",
        "\n",
        "    else:\n",
        "        # generate first image from prompt\n",
        "        #fn_check_pipeline()\n",
        "        text_pipeline.enable_model_cpu_offload() # this MASSIVELY speeds up inference\n",
        "        # next line doesn't work :(\n",
        "        #text_pipeline.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n",
        "        outimage = fn_generate(None, args.prompt, text_pipeline)\n",
        "        outpath = os.path.join(args.output_path, f'frame_{\"0\".zfill(4)}.png')\n",
        "        outimage.save(outpath)\n",
        "        src_image = outpath\n",
        "\n",
        "    \n",
        "    # Generate the images\n",
        "    tick = time.time_ns()\n",
        "\n",
        "    for frame_num in range(args.count + 1):\n",
        "        frame_id = str(frame_num + 1).zfill(4)\n",
        "        output_file_path = os.path.join(args.output_path, f\"frame_{frame_id}.png\")\n",
        "\n",
        "        print(f\"Generating image for {src_image} to {output_file_path}...\")\n",
        "\n",
        "        outimage = fn_generate(src_image, args.prompt, img2img_pipeline)\n",
        "        \n",
        "        #APPLY TRANSFORMS HERE\n",
        "        #=====================\n",
        "        outimage = ImageChops.offset(outimage, -2, 2)     # translate image\n",
        "        \n",
        "        # zoom, rotate, then crop to 512x512\n",
        "        outimage = outimage.resize((int(outimage.width * 1.03), int(outimage.height * 1.03)))\n",
        "        outimage = outimage.rotate(1)\n",
        "        outimage = outimage.crop((0, 0, 512, 512))\n",
        "        \n",
        "        outimage.save(output_file_path)     # save the transformed image\n",
        "        \n",
        "        src_image = output_file_path        # set the source image for the next iteration\n",
        "    \n",
        "    tock = time.time_ns()\n",
        "    baseline = f\"{(tock - tick) / 1e9:.1f}\" # convert to seconds\n",
        "    print(f\"Execution time -- {baseline} seconds\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# post-processing\n",
        "\n",
        "if True:  # args.gif:\n",
        "    images = []\n",
        "\n",
        "    # skip first frame, since it can be overpowering\n",
        "    # may want to skip first few frames\n",
        "    for i in range(1, args.count + 1):\n",
        "        images.append(\n",
        "            imageio.imread(\n",
        "                os.path.join(args.output_path, f\"frame_{str(i).zfill(4)}.png\")\n",
        "            )\n",
        "        )\n",
        "\n",
        "    imageio.mimsave(\"output.gif\", images, duration=1.0)\n",
        "    print(\"GIF saved to output.gif\")\n",
        "\n",
        "    if not args.no_interpolate:\n",
        "        # check if ffmpeg is available\n",
        "        if os.system(\"ffmpeg -version\") != 0:\n",
        "            print(\"ffmpeg not found, skipping interpolation\")\n",
        "            exit(1)\n",
        "\n",
        "        # interpolate the gif\n",
        "        os.system(\n",
        "            f\"ffmpeg -i output.gif -filter \\\"minterpolate='fps=60'\\\" output-interpolated-once.webm\"\n",
        "        )\n",
        "        os.system(\n",
        "            f'ffmpeg -stream_loop {LOOP_COUNT} -i output-interpolated-once.webm -c copy \"{args.prompt}.webm\"'\n",
        "        )\n",
        "\n",
        "        os.remove(\"output-interpolated-once.webm\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv_mycoding",
      "language": "python",
      "name": "venv_mycoding"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}