{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "runs stable diffusion via diffusers, creates a series of frames, with simplex noise distortions, and saves them as a video\n",
    "from original script by Dr47. Apr/May 2024\n",
    "'''\n",
    "#import libraries\n",
    "import os\n",
    "import time \n",
    "import argparse\n",
    "from PIL import Image, ImageChops\n",
    "\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "    \n",
    "import torch\n",
    "import imageio\n",
    "import sys      # provides access to some vars used or maintained by Python interpreter, and to functions that interact with it.\n",
    "\n",
    "from xformers.ops import MemoryEfficientAttentionFlashAttentionOp # Enable memory efficient attention from xFormers.(not working atm)\n",
    "\n",
    "import numpy as np\n",
    "from opensimplex import OpenSimplex # for noise generation\n",
    "\n",
    "#check cuda availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Using device:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model and pipeline\n",
    "\n",
    "#optimisation trick #1 - Use TensorFloat-32\n",
    "'''PyTorch enables TF32 mode for convolutions, but NOT for matmul. Enabling it can \n",
    "significantly (eg: 24% on RTX3070) speed up computations with minimal loss in numerical accuracy.\n",
    "'''\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "args = None\n",
    "\n",
    "# path to SD1.5 model\n",
    "SD15_MODEL = \"D:/SD_CKPTS_used_by_Auto1111/realisticVisionV51_v51VAE.safetensors\"\n",
    "LOOP_COUNT = 4 # Number of times to loop the final animation (results in loop count +1)\n",
    "\n",
    "# Define pipeline (half-precision weights for faster execution)\n",
    "def get_pipeline():\n",
    "    return (\n",
    "        StableDiffusionImg2ImgPipeline.from_single_file( SD15_MODEL, torch_dtype=torch.float16 ).to(\"cuda\"),\n",
    "        \n",
    "        AutoPipelineForText2Image.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16,\n",
    "            variant=\"fp16\", use_safetensors=True, safety_checker=None,   ).to(\"cuda\")    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to Generate the image\n",
    "\n",
    "def fn_generate(input_path, prompt, pipeline):\n",
    "    init_image = None\n",
    "    if input_path:\n",
    "        # Load the input image\n",
    "        init_image = Image.open(input_path).convert(\"RGB\")\n",
    "        \n",
    "    # Generate the output image\n",
    "    return pipeline( prompt=prompt, image=init_image, strength=args.strength,\n",
    "        num_inference_steps=args.num_inference_steps, guidance_scale=args.guidance_scale,\n",
    "        generator=( torch.Generator(device=\"cuda\").manual_seed(args.seed) if args.seed else None ) ).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_process_image(im):\n",
    "    '''takes an image, returns a distorted image'''\n",
    "    \n",
    "    np_im = np.array(im)\n",
    "    #get h and w of image\n",
    "    h, w, _ = np_im.shape # ignore the 3rd dimension (RGB)\n",
    "\n",
    "    # Create a new array to hold the distorted image data\n",
    "    distorted_data = np.empty_like(np_im)\n",
    "\n",
    "    # Distort image with Perlin noise\n",
    "    distort_intensity = 9\n",
    "    distort_scale = 60.0 # higher is smoother\n",
    "    seed = 123 # seed for noise\n",
    "\n",
    "    # Create arrays to hold the distortions\n",
    "    x_distort = np.empty((h,w), dtype=np.float64)\n",
    "    y_distort = np.empty((h,w), dtype=np.float64)\n",
    "\n",
    "    # Create a noise generator\n",
    "    gen = OpenSimplex(seed=seed)\n",
    "\n",
    "    # Generate Perlin noise and store the distortions\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            # Generate Perlin noise based on the pixel coordinates and the seed\n",
    "            n = gen.noise2(i / distort_scale, j / distort_scale)\n",
    "            \n",
    "            # Map the noise to [0, 1]\n",
    "            n = (n + 1) / 2\n",
    "            \n",
    "            # Store the distortions\n",
    "            x_distort[i, j] = n * distort_intensity\n",
    "            y_distort[i, j] = n * distort_intensity\n",
    "\n",
    "    # Calculate the mean distortions\n",
    "    x_mean_distort = np.mean(x_distort)\n",
    "    y_mean_distort = np.mean(y_distort)\n",
    "\n",
    "    # Apply distortions and translate image back to its (approx) original position\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            # distort pixel co-ords\n",
    "            x = int((i + x_distort[i, j] - x_mean_distort) % h)\n",
    "            y = int((j + y_distort[i, j] - y_mean_distort) % w)\n",
    "            \n",
    "            # Copy pixel data\n",
    "            distorted_data[i, j] = np_im[x, y]\n",
    "\n",
    "    # Create a new image from distorted data\n",
    "    distorted_img = Image.fromarray(distorted_data)\n",
    "\n",
    "    return distorted_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_check_pipeline():\n",
    "    # Check if the pipeline is working\n",
    "    img2img_pipeline = get_pipeline()\n",
    "    #print(f\"img2img_pipeline: {img2img_pipeline}\")\n",
    "    print(f\"img2img_pipeline: {str(img2img_pipeline)[:100]}\") # print the first 100 characters of the pipeline\n",
    "    \n",
    "    if img2img_pipeline is None:\n",
    "        raise ValueError(\"pipeline for img2img has issues\")\n",
    "    return\n",
    "\n",
    "fn_check_pipeline()     #test the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create argument parser\n",
    "class Args:\n",
    "    init_image = None\n",
    "    output_path = \"output-frames\"\n",
    "    count = 50  # number of frames to generate\n",
    "    prompt = \"a box of rocks\"  # replace with your value\n",
    "    strength = 0.5 # was 0.65\n",
    "    seed = None\n",
    "    num_inference_steps = 15\n",
    "    guidance_scale = 7\n",
    "    skip_frame_generation = False\n",
    "    gif = True\n",
    "    no_interpolate = False # disable motion interpolation for final ffmpeg post-processing\n",
    "    model = \"SD15\"\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_black_with_random_color(pixel): # does this work?\n",
    "    if pixel.all == (0, 0, 0):\n",
    "        return (np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255))\n",
    "    return pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function to generate the images\n",
    "\n",
    "if not args.skip_frame_generation:\n",
    "    img2img_pipeline, text_pipeline = get_pipeline()\n",
    "    print('pipelines have been gotten!')\n",
    "    if not os.path.exists(args.output_path):\n",
    "        os.makedirs(args.output_path)\n",
    "\n",
    "    if args.init_image:\n",
    "        # get initial image\n",
    "        src_image = args.init_image\n",
    "\n",
    "        # copy the initial image to the output path as frame 0\n",
    "        init_image = Image.open(src_image).convert(\"RGB\")\n",
    "        init_image = init_image.resize((512,512))\n",
    "        init_image.save(os.path.join(args.output_path, f'frame_{\"0\".zfill(4)}.png'))\n",
    "\n",
    "    else:\n",
    "        # generate first image from prompt\n",
    "        #fn_check_pipeline()\n",
    "        text_pipeline.enable_model_cpu_offload() # this MASSIVELY speeds up inference\n",
    "        # next line doesn't work :(\n",
    "        #text_pipeline.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n",
    "        outimage = fn_generate(None, args.prompt, text_pipeline)\n",
    "        outpath = os.path.join(args.output_path, f'frame_{\"0\".zfill(4)}.png')\n",
    "        outimage.save(outpath)\n",
    "        src_image = outpath\n",
    "\n",
    "    \n",
    "    # Generate the images\n",
    "    print ('starting the clock!...')\n",
    "    tick = time.time_ns()\n",
    "\n",
    "    for frame_num in range(args.count + 1):\n",
    "        frame_id = str(frame_num + 1).zfill(4)\n",
    "        output_file_path_and_name = os.path.join(args.output_path, f\"frame_{frame_id}.png\")\n",
    "\n",
    "        print(f\"Generating image for {src_image} to {output_file_path_and_name}...\")\n",
    "        outimage = fn_generate(src_image, args.prompt, img2img_pipeline)\n",
    "        \n",
    "        outimage.save(output_file_path_and_name)       # save the ORIGINAL image FIRST\n",
    "\n",
    "        #APPLY TRANSFORMS HERE\n",
    "        #=====================\n",
    "        outimage = fn_process_image(outimage)    \n",
    "\n",
    "        outimage.save(output_file_path_and_name)       # save the 3D rotated image\n",
    "        src_image = output_file_path_and_name          # set the source image for the next iteration\n",
    "    \n",
    "    tock = time.time_ns()\n",
    "    baseline = f\"{(tock - tick) / 1e9:.1f}\" # convert to seconds\n",
    "    print(f\"Execution time -- {baseline} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post-processing\n",
    "\n",
    "if True:  # args.gif:\n",
    "    images = []\n",
    "\n",
    "    # maybe skip first frame, since it can be overpowering? (all frames used for now)\n",
    "    for i in range(0, args.count + 1):\n",
    "        images.append( imageio.imread( os.path.join(args.output_path, f\"frame_{str(i).zfill(4)}.png\") ) )\n",
    "\n",
    "    imageio.mimsave(\"output.gif\", images, duration=1.0)\n",
    "    print(\"GIF saved to output.gif\")\n",
    "    imageio.mimsave(\"output.mp4\", images, fps=2)\n",
    "    print(\"MP4 saved to output.mp4\")\n",
    "\n",
    "    if not args.no_interpolate:\n",
    "        # check if ffmpeg is available\n",
    "        if os.system(\"ffmpeg -version\") != 0:\n",
    "            print(\"ffmpeg not found, skipping interpolation\")\n",
    "            exit(1)\n",
    "\n",
    "        # interpolate the gif - CARE on quotes formatting in the filter string...\n",
    "        print(\"Interpolating frames to 4fps...\")\n",
    "        os.system( f'ffmpeg -i output.mp4 -vf \"minterpolate=\\'mi_mode=mci:mc_mode=aobmc:vsbmc=1:fps=4\\'\" output_4fps.mp4' )\n",
    "        print(\"Interpolating frames to 8fps...\")\n",
    "        os.system( f'ffmpeg -i output_4fps.mp4 -vf \"minterpolate=\\'mi_mode=mci:mc_mode=aobmc:vsbmc=1:fps=8\\'\" output_8fps.mp4' )\n",
    "        print(f\"Looping final video a total of {LOOP_COUNT+1} times...\")\n",
    "        os.system( f'ffmpeg -stream_loop {LOOP_COUNT} -i output_8fps.mp4 -c copy \"{args.prompt}.mp4\"' )\n",
    "        print(\"Interpolation done. files saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
